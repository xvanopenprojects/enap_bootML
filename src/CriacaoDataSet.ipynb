{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCyYhw0KqGAk"
   },
   "source": [
    "# 1. Preparando ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfUhsS1Rqekt"
   },
   "source": [
    "## Colocando pasta dos dados como Raiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T12:55:31.584760Z",
     "start_time": "2024-10-22T12:54:09.171429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Cria uma SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"MinhaAppSpark\")\n",
    "         .master(\"local[*]\")  # \"local[*]\" usa todos os núcleos disponíveis localmente\n",
    "         .config(\"spark.ui.port\", \"4040\") \n",
    "         .getOrCreate()\n",
    "         )\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "# Verifique se a sessão foi criada\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T17:31:16.592513Z",
     "start_time": "2024-10-22T17:31:16.578460Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Adiciona o diretório atual ao Python path\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "   \n",
    "from libs.storage import Storage\n",
    "\n",
    "storage_log = Storage(\"files/sipe-u-log-hist-ate-2025\")\n",
    "storage_pefil = Storage(\"files/perfil\")\n",
    "storage_data = Storage(\"data\")\n",
    "storage_servidor = Storage(\"files/siape-servidor\")\n",
    "\n",
    "limit = 10000\n",
    "random_seed = 42\n",
    "storage_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opPmVibqqzos"
   },
   "source": [
    "# 2. Pré-Processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escolhendo os CPF\n",
    "\n",
    "- len(co_nivel_funcao ) = 3\n",
    "- de 1.01 ate 1.07 - chere\n",
    "- de 1.08 ate 1.10 - coordenador\n",
    "- de 1.11 ate 1.13 - coordenador geral\n",
    "- acima de 1.13 - diretor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, length, row_number, count, desc\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Carregar os dados da pasta usando o Spark\n",
    "df_cpfs = (\n",
    "    spark.read.parquet(storage_servidor.get_directory_name())\n",
    "    .where(length(col(\"co_nivel_funcao\")) == 3)  # Check length of co_nivel_funcao\n",
    "    .withColumn(\n",
    "        \"classificacao\",\n",
    "        when(col(\"co_nivel_funcao\").between(101, 107), \"Chefe\")\n",
    "        .when(col(\"co_nivel_funcao\").between(108, 110), \"Coordenador\")\n",
    "        .when(col(\"co_nivel_funcao\").between(111, 113), \"Coordenador Geral\")\n",
    "        .otherwise(\"Diretor\")\n",
    "    )\n",
    "    .where(\"da_obito = 0\")\n",
    "    .where(\"co_situacao_servidor = 1\")\n",
    "    .where(\"da_ocor_inatividade_serv = 0\")\n",
    "    .orderBy(desc(\"da_ocor_ingr_orgao_serv\"))\n",
    ")\n",
    "# Agrupando por classificação e amostrando 100 registros por grupo\n",
    "windowSpec = Window.partitionBy(\"classificacao\").orderBy(\"co_nivel_funcao\")  # Order by co_nivel_funcao\n",
    "df_amostra = df_cpfs.withColumn(\"row_number\", row_number().over(windowSpec)) \\\n",
    "                   .filter(col(\"row_number\") <= 130) \\\n",
    "                   .drop(\"row_number\")  # Remove the auxiliary column\n",
    "                   \n",
    "# removendo quem tece mais de 2 papeis\n",
    "windowSpec = Window.partitionBy(\"nu_cpf\")\n",
    "df_cpfs_count = df_amostra.withColumn(\"count\", count(\"*\").over(windowSpec))\n",
    "df_amostra = df_cpfs_count.where(col(\"count\") == 1).drop(\"count\")\n",
    "\n",
    "df_amostra.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpfs = df_amostra.groupBy([\"nu_cpf\", \"classificacao\"]).count().orderBy(\"count\", ascending=False).drop(\"count\")\n",
    "\n",
    "df_cpfs.groupBy([\"classificacao\"]).count().orderBy(\"count\", ascending=False).show()\n",
    "df_cpfs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfMin51GrIU1"
   },
   "source": [
    "### Carregando dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T12:55:44.538698Z",
     "start_time": "2024-10-22T12:55:31.620125Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZPr3WBiwp7-R",
    "outputId": "a87f474c-bade-4b14-95bc-db131f8a521f"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trim, col\n",
    "\n",
    "cpf_list = [row.nu_cpf for row in df_cpfs.select('nu_cpf').collect()]\n",
    "\n",
    "\n",
    "# Carregar os dados da pasta usando o Spark\n",
    "df = (\n",
    "    spark.read.parquet(storage_log.get_directory_name())\n",
    "    .withColumn('ch_arvore_senha_log', trim(col('ch_arvore_senha_log')))\n",
    "    .where(\"ch_arvore_senha_log <> 'SIAPE' or trim(ch_arvore_senha_log) <> ''\")\n",
    "    .where(\"trim(ch_arvore_senha_log) <> 'SIAPE     SIAPENET  ORGAO               LOGIN'\")\n",
    "    .where(\"nu_cpf_usuario_log <> '11111111111'\")\n",
    "    .where(col('nu_cpf_usuario_log').isin(cpf_list))  # Aqui aplicamos o filtro correto\n",
    "    .join(df_cpfs.select('nu_cpf', 'classificacao'), \n",
    "          col('nu_cpf_usuario_log') == col('nu_cpf'), \n",
    "          'left')  # Usar left join para garantir que todos os registros do df original estejam presentes\n",
    "    .limit(limit)\n",
    ")\n",
    "\n",
    "# Mostrar algumas informações sobre o DataFrame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyeu19EtwgOu"
   },
   "source": [
    "## Transformação 1\n",
    "\n",
    "Alterar a coluna 'ch_arvore_senha_log' em 5 novas colunas.\n",
    "\n",
    "Regra da expressão recular:\n",
    "1. SIGLA do Sistema\n",
    "1. [SUBSISTEMA]\n",
    "1. [MODULO]\n",
    "1. [OPCAO]\n",
    "1. [ATIVIDADE]\n",
    "1.  TRANSACAO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T12:55:45.135323Z",
     "start_time": "2024-10-22T12:55:44.541261Z"
    },
    "id": "8qyTgtJcxn8T"
   },
   "outputs": [],
   "source": [
    "# prompt: Transformar a coluna ch_arvore_senha_log em 6 colunas com a regra:\n",
    "# Regra da expressão recular:\n",
    "# - SIGLA_SISTEMA\n",
    "# - [SUBSISTEMA]\n",
    "# - [MODULO]\n",
    "# - [OPCAO]\n",
    "# - [ATIVIDADE]\n",
    "# - TRANSACAO\n",
    "\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "# Definir a expressão regular para extrair as informações da coluna 'ch_arvore_senha_log'\n",
    "regex = r\"^(\\w+)(?:\\s+(\\w+))?(?:\\s+(\\w+))?(?:\\s+(\\w+))?(?:\\s+(\\w+))?\\s+(\\w+)$\"\n",
    "\n",
    "# Criar novas colunas usando regexp_extract\n",
    "df = df.withColumn(\"SIGLA_SISTEMA\", regexp_extract(\"ch_arvore_senha_log\", regex, 1)) \\\n",
    "    .withColumn(\"SUBSISTEMA\", regexp_extract(\"ch_arvore_senha_log\", regex, 2)) \\\n",
    "    .withColumn(\"MODULO\", regexp_extract(\"ch_arvore_senha_log\", regex, 3)) \\\n",
    "    .withColumn(\"OPCAO\", regexp_extract(\"ch_arvore_senha_log\", regex, 4)) \\\n",
    "    .withColumn(\"ATIVIDADE\", regexp_extract(\"ch_arvore_senha_log\", regex, 5)) \\\n",
    "    .withColumn(\"TRANSACAO\", regexp_extract(\"ch_arvore_senha_log\", regex, 6))\n",
    "\n",
    "# Remover colunas que não tem sistema [não deveria ter nenhuma]\n",
    "df = df.dropna(how='any', subset=['SIGLA_SISTEMA', 'TRANSACAO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T12:55:45.208411Z",
     "start_time": "2024-10-22T12:55:45.142421Z"
    },
    "id": "GJLHgH6q4jdy"
   },
   "outputs": [],
   "source": [
    "# remover colunas: tx_dados_atualizados_log|tx_parametros_transacao\n",
    "# muita informação pessoal\n",
    "df = df.drop(\n",
    "    \"tx_dados_atualizados_log\", \n",
    "    \"tx_dados_atualizados_log_2\", \n",
    "    \"tx_parametros_transacao\",\n",
    "    \"nu_cpf\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T12:56:21.721140Z",
     "start_time": "2024-10-22T12:55:45.213975Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iT76HtnN8YYe",
    "outputId": "75cf1926-f5d3-4467-a799-16712b2c99f0"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Assuming 'df' is your PySpark DataFrame\n",
    "\n",
    "# Get a list of all columns in the DataFrame\n",
    "all_columns = df.columns\n",
    "\n",
    "# Create a dictionary to store the unique counts for each column\n",
    "unique_counts = {}\n",
    "\n",
    "# Iterate through each column and calculate the number of unique values\n",
    "for column in all_columns:\n",
    "    unique_counts[column] = df.select(countDistinct(column)).first()[0]\n",
    "\n",
    "# Print the unique counts for each column\n",
    "for column, count in unique_counts.items():\n",
    "    print(f\"Column '{column}': {count} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtY4h6TP68V0"
   },
   "source": [
    "## Transformaçao 2 - Anonimização\n",
    "\n",
    "Trocar os nomes dos orgão e os cpf por uma sequencia numérica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T12:56:21.737054Z",
     "start_time": "2024-10-22T12:56:21.724362Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5hNzM4Cy7Moc",
    "outputId": "877c66e1-7544-475f-c1ef-358b9207b032"
   },
   "outputs": [],
   "source": [
    "# prompt: como anonimizar dados de uma coluna? tenho algumas colunas para serem anonimizadas: co_orgao_log, matricula_inst_legal, co_orgao_servidor, nu_iden_unica_siape_log.\n",
    "# Gostaria que fosse guardada a transformação para poder fazer a inversa.  Os valores que se repetirem precisam ter os mesmos indices\n",
    "# 123 > 1\n",
    "# 124 > 2\n",
    "# 123 > 1\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Lista de colunas a serem anonimizadas\n",
    "colunas_anonimizar = [\n",
    "    'co_orgao_log',\n",
    "    'matricula_inst_legal',\n",
    "    'co_orgao_servidor',\n",
    "    'nu_cpf_usuario_log',\n",
    "    'nu_iden_unica_siape_log',\n",
    "    # 'ch_arvore_senha_log'\n",
    "]\n",
    "# colunas_anonimizar = []\n",
    "\n",
    "# Criar um dicionário para armazenar a transformação de cada coluna\n",
    "mapeamento_colunas = {}\n",
    "\n",
    "\n",
    "def create_mapped(df, colunas_anonimizar):\n",
    "    for coluna in colunas_anonimizar:\n",
    "        # Criar um novo DataFrame com uma coluna de ID única para cada valor da coluna a ser anonimizada\n",
    "        df_temp = df.select(coluna).distinct().withColumn(\"novo_valor\", monotonically_increasing_id() + 1)\n",
    "\n",
    "        # Salvar esse mapeamento em arquivo\n",
    "        df_temp.write.mode(\"overwrite\").parquet(storage_pefil.get_path(f\"mapeamento_{coluna}\"))\n",
    "        print(df_temp.show(5))\n",
    "\n",
    "        df = do_mapped(df, colunas_anonimizar, coluna, df_temp)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_mappped(df, colunas_anonimizar):\n",
    "    for coluna in colunas_anonimizar:\n",
    "        # Criar um novo DataFrame com uma coluna de ID única para cada valor da coluna a ser anonimizada\n",
    "        df_temp = spark.read.parquet(storage_pefil.get_path(f\"mapeamento_{coluna}\", True))\n",
    "\n",
    "        df = do_mapped(df, colunas_anonimizar, coluna, df_temp)\n",
    "    return df\n",
    "\n",
    "\n",
    "def do_mapped(df, colunas_anonimizar, coluna, df_temp):\n",
    "    global mapeamento_colunas\n",
    "    # Criar um dicionário com o mapeamento entre o valor original e o novo valor\n",
    "    mapeamento_colunas[coluna] = df_temp.rdd.map(lambda row: (row[coluna], row[\"novo_valor\"])).collectAsMap()\n",
    "    # Substituir os valores da coluna original pelos novos valores usando o mapeamento\n",
    "    df = df.replace(mapeamento_colunas[coluna], subset=[coluna])\n",
    "    return df\n",
    "\n",
    "print(colunas_anonimizar)\n",
    "df = create_mapped(df, colunas_anonimizar)\n",
    "# load_mappped(colunas_anonimizar)\n",
    "# df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Salvando df selecionado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T12:56:22.204078Z",
     "start_time": "2024-10-22T12:56:21.740128Z"
    }
   },
   "outputs": [],
   "source": [
    "storage_data.get_path(f\"dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T12:56:49.538681Z",
     "start_time": "2024-10-22T12:56:22.208416Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").parquet(storage_data.get_path(f\"dataset\"))"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "save_output": true,
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
