{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCyYhw0KqGAk"
   },
   "source": [
    "# 1. Preparando ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfUhsS1Rqekt"
   },
   "source": [
    "## Colocando pasta dos dados como Raiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T12:55:31.584760Z",
     "start_time": "2024-10-22T12:54:09.171429Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Cria uma SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"MinhaAppSpark\")\n",
    "         .master(\"local[*]\")  # \"local[*]\" usa todos os núcleos disponíveis localmente\n",
    "         .getOrCreate()\n",
    "         )\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "# Verifique se a sessão foi criada\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T17:31:16.592513Z",
     "start_time": "2024-10-22T17:31:16.578460Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Adiciona o diretório atual ao Python path\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "   \n",
    "from libs.storage import Storage\n",
    "\n",
    "storage_log = Storage(\"files/sipe-u-log-hist-ate-2025\")\n",
    "storage_pefil = Storage(\"files/perfil\")\n",
    "storage_data = Storage(\"data\")\n",
    "storage_servidor = Storage(\"files/siape-servidor\")\n",
    "\n",
    "limit = 10000\n",
    "random_seed = 42\n",
    "storage_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregar o df selecionado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(storage_data.get_path(f\"dataset.csv\"))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_lp2Q7asDq5"
   },
   "source": [
    "## Transformação 3\n",
    "\n",
    "Transformar o log normal em uma matrix incidência.\n",
    "\n",
    "T = Transação [ch_arvore_senha_log]\n",
    "\n",
    "A matriz a ser criada é TxT porém com os seguintes detalhes.\n",
    "Agrupar por [CRITÉRIOS]\n",
    "\n",
    "CRITÉRIOS:\n",
    "1. Por órgão: co_orgao_log\n",
    "2. Por órgão do servidor: co_orgao_servidor\n",
    "3. Por ano ou mês ou dia: da_atualizacao_log\n",
    "\n",
    "### Explicação do processo:\n",
    "\n",
    "Para transformar em uma matrix incidência. Vamos rodar todas as transações que um CPF fez por dia.\n",
    "\n",
    "Se o log regostrou A em um horário e depois B num horário subsequente no mesmo dia. Vamos crira um aresta, A -> B, com o valor 1, se voltar para A e de A for para B, então a aresta A->B agora vale 2. Se no dia subsequente ele saiu novamente de A -> B, então o valor passa a ser 3.\n",
    "\n",
    "Lembrando que A -> B é diferente de B -> A. Ou seja, estamos criando um DAG, grafo direcionado.\n",
    "\n",
    "Serão criados n grafos de acordo com os agrupadores: 'co_orgao_log', 'matricula_inst_legal', 'co_orgao_servidor', 'nu_cpf_usuario_log', 'nu_iden_unica_siape_log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_graph(graph, origem, destino):\n",
    "    if origem not in graph:\n",
    "        graph[origem] = {}\n",
    "\n",
    "    if destino not in graph[origem]:\n",
    "        graph[origem][destino] = 1\n",
    "    else:\n",
    "        graph[origem][destino] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph(df_main):\n",
    "    index_cpf = 'nu_cpf_usuario_log'\n",
    "    index_data = 'da_atualizacao_log'\n",
    "    index_transacao = \"TRANSACAO\"\n",
    "    \n",
    "    tmp_graph = {}\n",
    "\n",
    "    df_indice_cpf = df_main.groupBy(index_cpf).count().orderBy(\"count\", ascending=False)\n",
    "    # print(\"df_indice_cpf\", df_indice_cpf.head(5))\n",
    "    for row_cpf in df_indice_cpf.collect():\n",
    "        # print(row_cpf)\n",
    "        cpf = row_cpf[index_cpf]\n",
    "        # print(\"cpf\", cpf)\n",
    "        df_tmp_cpf = df_main.filter(df_main[index_cpf] == cpf)\n",
    "        # print(\"df_tmp_cpf\", df_tmp_cpf.count())\n",
    "\n",
    "        df_indice_data = df_tmp_cpf.groupBy(index_data).count().orderBy(\"count\", ascending=False)\n",
    "        # print(\"df_indice_data\", df_indice_data.count())\n",
    "\n",
    "        # Iterate over unique data of 'cpf','sistema'\n",
    "        for row_data in df_indice_data.collect():  # Iterate over pandas DataFrame\n",
    "            # print(\"data\", row_data)\n",
    "            data = row_data[index_data]\n",
    "            df_tmp_acao = df_tmp_cpf.filter(df_tmp_cpf[index_data] == data)\n",
    "            # print(\"df_tmp_acao\", df_tmp_acao.count())\n",
    "\n",
    "            if df_tmp_acao.count() < 2:\n",
    "                continue\n",
    "            count = 1\n",
    "            # Iterate over unique 'acao' of 'data','cpf','sistema'\n",
    "            for row_acao in df_tmp_acao.collect():  # Iterate over pandas DataFrame\n",
    "                # progress.next()\n",
    "                destino = row_acao[index_transacao]\n",
    "                if destino in [\n",
    "                    'SIAPE     SIAPENET  ORGAO               LOGIN', \n",
    "                    'SIAPE', \n",
    "                    '', \n",
    "                    'LOGIN'\n",
    "                ]:\n",
    "                    continue\n",
    "                # Dividir o texto em uma lista de palavras\n",
    "                # origem = re.findall(pattern, origem)\n",
    "                if count > 1:\n",
    "                    # palavras = [destino[i:i+10] for i in range(0, len(destino), 10)]\n",
    "                    # destino = palavras[-1]\n",
    "                    fill_graph(tmp_graph, origem, destino)\n",
    "\n",
    "                origem = destino\n",
    "\n",
    "                count += 1\n",
    "\n",
    "    return tmp_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph_by(df, index, index_name):\n",
    "    graph = {}\n",
    "    df_index = df.groupBy(index).count().orderBy(\"count\", ascending=False)\n",
    "    print(index_name, df_index.count())\n",
    "\n",
    "    for row_sistema in df_index.collect():\n",
    "        # print(row_sistema)\n",
    "        camada = row_sistema[index]\n",
    "        # print(\"sistema\", sistema)\n",
    "        df_tmp_main = df.filter(df[index] == camada)\n",
    "        # print(\"df_tmp_sistema\", df_tmp_sistema.count())\n",
    "\n",
    "        # Iterate over unique 'cpf' of 'sistema'\n",
    "        graph[f\"{index_name}_{camada}\"] = generate_graph(df_tmp_main)\n",
    "\n",
    "    # loberar memoria do df_tmp_main\n",
    "    df_tmp_main.unpersist()\n",
    "    df_index.unpersist()\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando o grafo filtrado por Sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializando o grafo\n",
    "graph = {}\n",
    "\n",
    "# Sistema\n",
    "index = 'SIGLA_SISTEMA'\n",
    "index_name = 'sistema'\n",
    "graph.update(generate_graph_by(df, index, index_name))\n",
    "\n",
    "# Órgão\n",
    "index = 'co_orgao_log'\n",
    "index_name = 'orgao'\n",
    "graph.update(generate_graph_by(df, index, index_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando o grafo em um arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T17:22:26.458554Z",
     "start_time": "2024-10-22T17:22:26.434737Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "graph_file = storage_pefil.get_path(\"graph.json\")\n",
    "# Abrindo um arquivo para escrita\n",
    "with open(graph_file, 'w') as arquivo:\n",
    "    json.dump(graph, arquivo, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando o grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "graph_file = storage_pefil.get_path(\"graph.json\")\n",
    "\n",
    "with open(graph_file, 'r') as arquivo:\n",
    "    graph = json.load(arquivo)\n",
    "\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desenhando o grafo no formato dot e salvando em um arquivo pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T17:23:21.309853Z",
     "start_time": "2024-10-22T17:23:19.115134Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "ZaP6nEnLrY98",
    "outputId": "39e18769-4945-4a69-9aa8-3164166765be"
   },
   "outputs": [],
   "source": [
    "import graphviz\n",
    "\n",
    "\n",
    "for camada in graph:\n",
    "    dot = graphviz.Graph(comment='Autorizações')\n",
    "    for source in graph[camada]:\n",
    "        # label = origem[:-20]\n",
    "        dot.node(source, label=source)\n",
    "        for target in graph[camada][source]:\n",
    "            dot.node(target, label=target)\n",
    "            dot.edge(source, target, label=\"%d\" % graph[camada][source][target])\n",
    "    dot.render(f'autoriza_{camada}.dot')  # Renderizar e abrir a imagem\n",
    "\n",
    "# print(dot.source)  # Visualizar o código DOT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T14:42:24.687044Z",
     "start_time": "2024-10-22T14:42:19.401191Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "adjacence_matrix = {}\n",
    "dicionario = {}\n",
    "collums = set()\n",
    "for camada in graph:\n",
    "    for source in graph[camada]:\n",
    "        for target in graph[camada][source]:\n",
    "            collums.add(source)\n",
    "            collums.add(target)\n",
    "    if camada not in adjacence_matrix:\n",
    "        adjacence_matrix[camada] = {}    \n",
    "    \n",
    "    adjacence_matrix[camada] = np.zeros((len(collums), len(collums)))\n",
    "    dicionario[camada] = dict(zip(collums, range(len(collums))))\n",
    "    # print(dicionario[sistema])\n",
    "\n",
    "for camada in graph:\n",
    "    d = dicionario[camada]\n",
    "    for source in graph[camada]:\n",
    "        for target in graph[camada][source]:\n",
    "            adjacence_matrix[camada][d[source]][d[target]] = graph[camada][source][target]\n",
    "adjacence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T17:33:28.007643Z",
     "start_time": "2024-10-22T17:33:26.867798Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "for camada in adjacence_matrix:\n",
    "    print(camada)\n",
    "    # Gerar dados de exemplo\n",
    "    X = adjacence_matrix[camada]\n",
    "    k_values = range(1, int(1.5*math.sqrt(X.shape[1])))\n",
    "    inercia = []\n",
    "    for k in k_values:\n",
    "        print(\"k\", k)\n",
    "        # Criar o modelo de clusterização K-Means\n",
    "        kmeans = KMeans(n_clusters=k, random_state=random_seed)\n",
    "        kmeans.fit(X)\n",
    "        \n",
    "        # Prever os clusters\n",
    "        y_kmeans = kmeans.predict(X)\n",
    "        print(y_kmeans)\n",
    "        inercia.append(kmeans.inertia_)\n",
    "    \n",
    "    # Plotar a inércia para cada valor de k\n",
    "    plt.plot(k_values, inercia, marker='o')\n",
    "    plt.xlabel('Número de Clusters (k)')\n",
    "    plt.ylabel('Inércia (Soma dos Erros Quadrados)')\n",
    "    plt.title('Método do Cotovelo para Escolha de k')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Exemplo de uma matriz de incidência (5 objetos com 4 características)\n",
    "matriz_incidencia = np.array([\n",
    "    [1, 0, 0, 1],  # Objeto 1\n",
    "    [1, 1, 0, 0],  # Objeto 2\n",
    "    [0, 1, 1, 0],  # Objeto 3\n",
    "    [1, 0, 0, 1],  # Objeto 4\n",
    "    [0, 1, 1, 1],  # Objeto 5\n",
    "])\n",
    "\n",
    "# Número de clusters (definido pelo usuário)\n",
    "num_clusters = 5\n",
    "\n",
    "# Criar o modelo K-Means\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "# Ajustar o modelo à matriz de incidência\n",
    "kmeans.fit(matriz_incidencia)\n",
    "\n",
    "# Prever os clusters\n",
    "rótulos = kmeans.predict(matriz_incidencia)\n",
    "\n",
    "# Exibir os rótulos dos clusters\n",
    "print(\"Rótulos dos clusters:\", rótulos)\n",
    "\n",
    "# Exibir os centróides dos clusters\n",
    "print(\"Centróides dos clusters:\", kmeans.cluster_centers_)\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "save_output": true,
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
